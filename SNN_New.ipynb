{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from modularities_notebooks.modularity import clustered_connections, print_weight_matrix, compute_density_matrix, plot_connection_matrices\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "if '/Users/svenkerstjens/DataSpell/lib/python3.9/site-packages' not in sys.path:\n",
    "    sys.path.append('/Users/svenkerstjens/DataSpell/lib/python3.9/site-packages')\n",
    "import torch\n",
    "import snn_utils\n",
    "from randman.randman import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from randman_utils import *\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Optional\n",
    "\n",
    "from LIFlayer import NHiddenModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kerstjens/msc_thesis/msc_thesis/SNN_New.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu4/Users/kerstjens/msc_thesis/msc_thesis/SNN_New.ipynb#ch0000001vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu4/Users/kerstjens/msc_thesis/msc_thesis/SNN_New.ipynb#ch0000001vscode-remote?line=1'>2</a>\u001b[0m     device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu4/Users/kerstjens/msc_thesis/msc_thesis/SNN_New.ipynb#ch0000001vscode-remote?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdevice: \u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'device: {device}')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandmanDataset(Dataset):\n",
    "    \"\"\"Characterizes a PyTorch dataset for use with the PyTorch dataloader.\"\"\"\n",
    "    def __init__(self, data, labels):\n",
    "        \"\"\"Simple initialization of the given dataset.\"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Retrives a single sample from the given dataset.\"\"\"\n",
    "        # Load data and get label\n",
    "        X = self.data[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "def convert_spike_times_to_raster(spike_times: np.ndarray, timestep: float = 1.0, max_time: Optional[float] = None, num_neurons: Optional[int] = None, dtype=None):\n",
    "    \"\"\"\n",
    "    Convert spike times array to spike raster array. \n",
    "    For now, all neurons need to have same number of spike times.\n",
    "    \n",
    "    Args:\n",
    "        spike_times: MoreArrays, spiketimes as array of shape (batch_dim x spikes/neuron X 2)\n",
    "            with final dim: (times, neuron_id)\n",
    "    \"\"\"\n",
    "\n",
    "    if dtype is None:\n",
    "        dtype = np.int16\n",
    "    # spike_times = spike_times.astype(np.uint16)\n",
    "    if num_neurons is None:\n",
    "        num_neurons = int(np.nanmax(spike_times[:,:,1]))+1\n",
    "    if max_time is None:\n",
    "        max_time = np.nanmax(spike_times[:,:,0])\n",
    "    num_bins = int(max_time / timestep + 1)\n",
    "\n",
    "    spike_raster = np.zeros((spike_times.shape[0], num_bins, num_neurons), dtype=np.float32)\n",
    "    batch_id = np.arange(spike_times.shape[0]).repeat(spike_times.shape[1])\n",
    "    spike_times_flat = (spike_times[:, :, 0].flatten() / timestep).astype(dtype)\n",
    "    neuron_ids = spike_times[:, :, 1].flatten().astype(dtype)\n",
    "    np.add.at(spike_raster, (batch_id, spike_times_flat, neuron_ids), 1)\n",
    "    return spike_raster\n",
    "\n",
    "\n",
    "def make_spike_raster_dataset(nb_classes=10, nb_units=100, nb_steps=100, dim_manifold=2, nb_samples=1000, alpha=2.0, shuffle=True, seed=None):\n",
    "# def make_spike_raster_dataset(nb_classes, nb_units, nb_steps, dim_manifold=2, nb_spikes=1, nb_samples=1000, alpha=2.0, shuffle=True):\n",
    "# def make_spike_raster_dataset():\n",
    "    spike_times,labels = make_spiking_dataset(nb_classes=nb_classes, nb_units=nb_units, nb_steps=nb_steps, dim_manifold=dim_manifold, seed=seed,nb_samples=nb_samples,shuffle=shuffle,alpha=alpha)\n",
    "    spike_raster = convert_spike_times_to_raster(spike_times)\n",
    "    return spike_raster, labels\n",
    "\n",
    "def get_data_loaders(nb_classes, nb_units, nb_steps, nb_samples, batchsize):\n",
    "    data, labels = make_spike_raster_dataset(nb_classes=nb_classes, nb_units=nb_units, nb_steps=nb_steps, nb_samples=nb_samples)\n",
    "\n",
    "    NUM_SAMPLES_TOTAL = (nb_classes*nb_samples)\n",
    "    NUM_SAMPLES_TRAIN = int(NUM_SAMPLES_TOTAL*0.8)\n",
    "\n",
    "    data_train, labels_train = data[:NUM_SAMPLES_TRAIN], labels[:NUM_SAMPLES_TRAIN]\n",
    "    data_test,  labels_test  = data[NUM_SAMPLES_TRAIN:], labels[NUM_SAMPLES_TRAIN:]\n",
    "\n",
    "    dataset_train = RandmanDataset(data_train, labels_train)\n",
    "    dataset_test = RandmanDataset(data_test, labels_test)\n",
    "\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batchsize, shuffle=True, drop_last=True)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=batchsize, shuffle=True, drop_last=True)\n",
    "    return dataloader_train, dataloader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W,W2 = get_Ws(200,4,1,.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_from_pred(prediction,y):\n",
    "\n",
    "    return (y == prediction.argmax(axis=1)).sum()/len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_data_loaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kerstjens/msc_thesis/msc_thesis/SNN_New.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu4/Users/kerstjens/msc_thesis/msc_thesis/SNN_New.ipynb#ch0000003vscode-remote?line=0'>1</a>\u001b[0m dataloader_train, dataloader_test \u001b[39m=\u001b[39m get_data_loaders(\u001b[39m4\u001b[39m,\u001b[39m200\u001b[39m,\u001b[39m100\u001b[39m,\u001b[39m1000\u001b[39m,\u001b[39m50\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_data_loaders' is not defined"
     ]
    }
   ],
   "source": [
    "dataloader_train, dataloader_test = get_data_loaders(4,200,100,1000,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train(N,T,nb_classes,model,X_train,y_train,train_batch_ids,W,W2,lr,decay,epochs=100,use_W=False,use_W2=False,print_each=5,device='cpu'):\n",
    "def train(T,dataloader_train,nb_classes,model,W,W2,lr,decay=.99,epochs=100,use_W=False,use_W2=False,print_each=5,device=device):\n",
    "    #in_channels=N\n",
    "    #hidden_channels = N\n",
    "    #out_channels = nb_classes\n",
    "    #model = FiveHiddenModel(in_channels,hidden_channels,out_channels,batch_size=batch_size,W=W)\n",
    "\n",
    "    #mse_loss = torch.nn.MSELoss()\n",
    "    ce_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    #apply W\n",
    "    if use_W:\n",
    "        model.init_mod_weights(W)\n",
    "\n",
    "    #layer1.fc_layer.weight = torch.nn.Parameter(torch.Tensor(W))\n",
    "    params = model.parameters()\n",
    "    opt = torch.optim.Adam(params, lr=lr, betas=[0., .95]) #lr is the learning rate\n",
    "    loss_hist = []\n",
    "    acc_hist = []\n",
    "    #epochs\n",
    "    for e in range(epochs):\n",
    "        batch_loss = 0\n",
    "        batch_acc=0\n",
    "        model.train()\n",
    "\n",
    "        #batches\n",
    "\n",
    "        #if e%10 == 0:\n",
    "        #    y = model.layer1.fc_layer.weight.clone().detach()\n",
    "        #    plt.imshow(y)\n",
    "        #    plt.show()\n",
    "        #plt.imshow(W)\n",
    "        #plt.show()\n",
    "        count = 0\n",
    "        for x,label in dataloader_train:\n",
    "            model.init_states()\n",
    "            Sprobe = torch.zeros((dataloader_train.batch_size,model.out_channels),device=device)\n",
    "            x = x.to(device)\n",
    "            label=label.to(device)\n",
    "            for n in range(T):\n",
    "                out_state = model(x[:,n])\n",
    "\n",
    "                Sprobe = decay * Sprobe + out_state.S\n",
    "\n",
    "            \n",
    "            prediction = Sprobe\n",
    "            accuracy = acc_from_pred(prediction,label)\n",
    "            loss = ce_loss(prediction,label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "#        for i in range(len(train_batch_ids)):\n",
    "#        \n",
    "#\n",
    "#            model.init_states()\n",
    "#            Sprobe = torch.zeros((batch_size,model.out_channels),device=device)\n",
    "#            #out = torch.zeros((batch_size,model.out_channels),device=device)\n",
    "#            #out_rec = [out]\n",
    "#        #timesteps\n",
    "#            for n in range(T):\n",
    "#                out_state = model(X_train[train_batch_ids[i],n])\n",
    "#\n",
    "#            #add decay for leakiness\n",
    "#            #collect spikes over time\n",
    "#                Sprobe = decay * Sprobe + out_state.S\n",
    "#                #out_rec.append(out_state.U)\n",
    "#\n",
    "#            #out_rec = torch.stack(out_rec,dim=1)\n",
    "#            #prediction = torch.max(out_rec,1).values\n",
    "#            prediction = Sprobe\n",
    "#            accuracy = acc_from_pred(prediction,y_train[train_batch_ids[i]])\n",
    "#            loss = ce_loss(prediction,y_train[train_batch_ids[i]])\n",
    "#\n",
    "#            loss.backward()\n",
    "#            opt.step()\n",
    "#            opt.zero_grad()\n",
    "            if use_W2:\n",
    "                model.init_mod_weights(W2)\n",
    "            count+=1\n",
    "            batch_acc = batch_acc + accuracy\n",
    "            batch_loss = batch_loss + loss\n",
    "        #epoch_acc = batch_acc/(len(train_batch_ids))\n",
    "        epoch_acc = batch_acc/count\n",
    "        epoch_loss = batch_loss/count\n",
    "        #epoch_loss = batch_loss/(len(train_batch_ids))\n",
    "        loss_hist = loss_hist + [float(epoch_loss)]\n",
    "        acc_hist = acc_hist + [float(epoch_acc)]\n",
    "\n",
    "\n",
    "        if e%print_each==0:\n",
    "            print(f'epoch {e}: \\n loss: {epoch_loss}')\n",
    "            print(f'train_acc: {epoch_acc}')\n",
    "    return loss_hist,acc_hist,Sprobe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Ws(N,nb_classes,epsilon=1, modularity =1,base_w = 1,w = 1,plot=True,device=device):\n",
    "    n_assemblies = nb_classes\n",
    "    ss = StandardScaler()\n",
    "    mask, cluster_ids, _ = clustered_connections(n_neurons=N, n_clusters=n_assemblies, density=1./n_assemblies, modularity=modularity)\n",
    "    mask = (mask* (base_w*w)) + base_w\n",
    "\n",
    "    W, cluster_ids, t = clustered_connections(n_neurons=N, n_clusters=n_assemblies, density=epsilon, modularity=0.)\n",
    "    if w > 0.:\n",
    "        W *= base_w\n",
    "        W *= mask\n",
    "\n",
    "    if epsilon ==1:\n",
    "        W = W-1\n",
    "    W2 = (W != 0).astype(int)\n",
    "    if plot:\n",
    "        plt.title('W')\n",
    "        plt.imshow(W)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        plt.title('W2')\n",
    "        plt.imshow(W2)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    return torch.tensor(W,device=device,dtype=torch.float32), torch.tensor(W2,device=device,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NHiddenModel(num_hidden_layers=1,in_channels=200,hidden_channels=200,out_channels=4,with_recurrent=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc,Sprobe = train(T=100,dataloader_train=dataloader_train,nb_classes=4,epochs=1, model=model,use_W=False,use_W2=False,lr=1e-4,decay=.99,W=W,W2=W2,print_each=1,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do something with forward hooks"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa49628190029e2d0794fd6aac1f35e1d10c3ca2aaa40697d2a44a2aa6659ca3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
