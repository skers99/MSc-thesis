{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'heidel_utils' from '/Users/kerstjens/msc_thesis/msc_thesis/heidel_utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "import heidel_utils\n",
    "from importlib import reload # reload \n",
    "reload(heidel_utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[97, 14, 12,  ..., 22, 78,  3],\n",
       "        [36, 82, 60,  ..., 35, 50, 80],\n",
       "        [57, 99, 21,  ..., 26, 68, 51],\n",
       "        ...,\n",
       "        [89, 24,  5,  ..., 59, 90, 96],\n",
       "        [98, 64,  8,  ..., 78, 65, 88],\n",
       "        [14, 18, 23,  ..., 12, 72, 59]], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(1,100,(100,200))\n",
    "a.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "NeuronState = namedtuple('NeuronState', ['U', 'I', 'S'])\n",
    "\n",
    "class LIFDensePopulation(nn.Module):\n",
    "    # NeuronState = namedtuple('NeuronState', ['U', 'I', 'S'])\n",
    "    def __init__(self, in_channels, out_channels, bias=True, alpha = .9, beta=.85, batch_size=10,W=None):\n",
    "        super(LIFDensePopulation, self).__init__()\n",
    "        self.fc_layer = nn.Linear(in_channels, out_channels)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.state = NeuronState(U=torch.zeros(1, out_channels).to(device),\n",
    "                                 I=torch.zeros(1, out_channels).to(device),\n",
    "                                 S=torch.zeros(1, out_channels).to(device))\n",
    "        self.NeuronState = self.state\n",
    "        self.fc_layer.weight.data.uniform_(-.3, .3)\n",
    "        self.fc_layer.bias.data.uniform_(-.01, .01)\n",
    "\n",
    "\n",
    "    def forward(self, Sin_t):\n",
    "        state = self.state\n",
    "        U = self.alpha*state.U + state.I - state.S\n",
    "        I = self.beta*state.I + self.fc_layer(Sin_t)\n",
    "        # update the neuronal state\n",
    "        S = smooth_step(U)\n",
    "        self.state = NeuronState(U=U, I=I, S=S)\n",
    "        self.NeuronState = self.state\n",
    "        #state = NeuronState(U=U, I=I, S=S)\n",
    "        return self.state\n",
    "\n",
    "    def init_state(self):\n",
    "\n",
    "        out_channels = self.out_channels\n",
    "        self.state = NeuronState(U=torch.zeros(self.batch_size, out_channels,device=device),\n",
    "                                 I=torch.zeros(self.batch_size, out_channels,device=device),\n",
    "                                 S=torch.zeros(self.batch_size, out_channels,device=device))\n",
    "        self.NeuronState = self.state\n",
    "\n",
    "    def init_mod_weights(self,W):\n",
    "        self.fc_layer.weight = torch.nn.Parameter(self.fc_layer.weight.data * torch.Tensor(W))\n",
    "\n",
    "class SmoothStep(torch.autograd.Function):\n",
    "    '''\n",
    "    Modified from: https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def forward(aux, x):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which\n",
    "        we need to later backpropagate our error signals. To achieve this we use the\n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        aux.save_for_backward(x)\n",
    "        out = torch.zeros_like(x)\n",
    "\n",
    "        out[x > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    def backward(aux, grad_output):\n",
    "        #grad_input = grad_output.clone()\n",
    "        input, = aux.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input <= -.5] = 0\n",
    "        grad_input[input > .5] = 0\n",
    "        return grad_input\n",
    "\n",
    "smooth_step = SmoothStep().apply\n",
    "\n",
    "class OneHiddenModel(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channels,hidden_channels,out_channels,batch_size,alpha=.9,beta=.85,device='cpu',W=None):\n",
    "        super(OneHiddenModel, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.W = W\n",
    "        self.layer1 = LIFDensePopulation(in_channels=self.in_channels,out_channels=self.hidden_channels,\n",
    "                                         alpha=self.alpha,beta=self.beta,batch_size=self.batch_size,W=W).to(device)\n",
    "        self.layer2 = LIFDensePopulation(in_channels=self.hidden_channels,out_channels=self.out_channels,\n",
    "                                         alpha=self.alpha,beta=self.beta,batch_size=self.batch_size).to(device)\n",
    "\n",
    "    def forward(self,Sin):\n",
    "        hidden = self.layer1(Sin)\n",
    "        out = self.layer2(hidden.S)\n",
    "        return out\n",
    "\n",
    "    def init_states(self):\n",
    "        self.layer1.init_state()\n",
    "        self.layer2.init_state()\n",
    "\n",
    "    def init_mod_weights(self,W):\n",
    "        self.layer1.fc_layer.weight = torch.nn.Parameter(self.layer1.fc_layer.weight.data * torch.Tensor(W))\n",
    "\n",
    "class ThreeHiddenModel(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channels,hidden_channels,out_channels,batch_size,alpha=.9,beta=.85,device='cpu',W=None):\n",
    "        super(ThreeHiddenModel, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.W = W\n",
    "        self.layer1 = LIFDensePopulation(in_channels=self.in_channels,out_channels=self.hidden_channels,\n",
    "                                         alpha=self.alpha,beta=self.beta,batch_size=self.batch_size,W=W).to(device)\n",
    "        self.layer2 = LIFDensePopulation(in_channels=self.hidden_channels,out_channels=self.hidden_channels,\n",
    "                                         alpha=self.alpha,beta=self.beta,batch_size=self.batch_size).to(device)\n",
    "        self.layer3 = LIFDensePopulation(in_channels=self.hidden_channels,out_channels=self.hidden_channels,\n",
    "                                         alpha=self.alpha,beta=self.beta,batch_size=self.batch_size).to(device)\n",
    "        self.layer4 = LIFDensePopulation(in_channels=self.hidden_channels,out_channels=self.hidden_channels,\n",
    "                                         alpha=self.alpha,beta=self.beta,batch_size=self.batch_size).to(device)\n",
    "        self.layer5 = LIFDensePopulation(in_channels=self.hidden_channels,out_channels=self.out_channels,\n",
    "                                         alpha=self.alpha,beta=self.beta,batch_size=self.batch_size).to(device)\n",
    "\n",
    "    def forward(self,Sin):\n",
    "        hidden1 = self.layer1(Sin)\n",
    "        hidden2 = self.layer2(hidden1.S)\n",
    "        hidden3 = self.layer3(hidden2.S)\n",
    "        hidden4 = self.layer4(hidden3.S)\n",
    "        out = self.layer5(hidden4.S)\n",
    "        return out\n",
    "\n",
    "    def init_states(self):\n",
    "        self.layer1.init_state()\n",
    "        self.layer2.init_state()\n",
    "        self.layer3.init_state()\n",
    "        self.layer4.init_state()\n",
    "        self.layer5.init_state()\n",
    "\n",
    "    def init_mod_weights(self,W):\n",
    "        self.layer1.fc_layer.weight = torch.nn.Parameter(self.layer1.fc_layer.weight.data * torch.Tensor(W))\n",
    "        self.layer2.fc_layer.weight = torch.nn.Parameter(self.layer2.fc_layer.weight.data * torch.Tensor(W))\n",
    "        self.layer3.fc_layer.weight = torch.nn.Parameter(self.layer3.fc_layer.weight.data * torch.Tensor(W))\n",
    "        self.layer4.fc_layer.weight = torch.nn.Parameter(self.layer4.fc_layer.weight.data * torch.Tensor(W))\n",
    "\n",
    "\n",
    "class FiveHiddenModel(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channels,hidden_channels,out_channels,batch_size,alpha=.9,beta=.85,device='cpu',W=None):\n",
    "        super(FiveHiddenModel, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.W = W\n",
    "        self.layer1 = LIFDensePopulation(in_channels=self.in_channels,out_channels=self.hidden_channels,\n",
    "                                         alpha=self.alpha,beta=self.beta,batch_size=self.batch_size,W=W).to(device)\n",
    "        self.layer2 = LIFDensePopulation(in_channels=self.hidden_channels,out_channels=self.hidden_channels,\n",
    "                                         alpha=self.alpha,beta=self.beta,batch_size=self.batch_size).to(device)\n",
    "        self.layer3 = LIFDensePopulation(in_channels=self.hidden_channels,out_channels=self.hidden_channels,\n",
    "                                         alpha=self.alpha,beta=self.beta,batch_size=self.batch_size).to(device)\n",
    "        self.layer4 = LIFDensePopulation(in_channels=self.hidden_channels,out_channels=self.hidden_channels,\n",
    "                                         alpha=self.alpha,beta=self.beta,batch_size=self.batch_size).to(device)\n",
    "        self.layer5 = LIFDensePopulation(in_channels=self.hidden_channels,out_channels=self.hidden_channels,\n",
    "                                        alpha=self.alpha,beta=self.beta,batch_size=self.batch_size).to(device)\n",
    "        self.layer6 = LIFDensePopulation(in_channels=self.hidden_channels,out_channels=self.hidden_channels,\n",
    "                                         alpha=self.alpha,beta=self.beta,batch_size=self.batch_size).to(device)\n",
    "        self.layer7 = LIFDensePopulation(in_channels=self.hidden_channels,out_channels=self.out_channels,\n",
    "                                         alpha=self.alpha,beta=self.beta,batch_size=self.batch_size).to(device)\n",
    "        #maybe try last layer non- spiking\n",
    "\n",
    "    def forward(self,Sin):\n",
    "        hidden1 = self.layer1(Sin)\n",
    "        hidden2 = self.layer2(hidden1.S)\n",
    "        hidden3 = self.layer3(hidden2.S)\n",
    "        hidden4 = self.layer4(hidden3.S)\n",
    "        hidden5 = self.layer5(hidden4.S)\n",
    "        hidden6 = self.layer6(hidden5.S)\n",
    "        out = self.layer7(hidden6.S)\n",
    "        return out\n",
    "\n",
    "    def init_states(self):\n",
    "        self.layer1.init_state()\n",
    "        self.layer2.init_state()\n",
    "        self.layer3.init_state()\n",
    "        self.layer4.init_state()\n",
    "        self.layer5.init_state()\n",
    "        self.layer6.init_state()\n",
    "        self.layer7.init_state()\n",
    "\n",
    "    def init_mod_weights(self,W):\n",
    "        self.layer1.fc_layer.weight = torch.nn.Parameter(self.layer1.fc_layer.weight.data * torch.Tensor(W))\n",
    "        self.layer2.fc_layer.weight = torch.nn.Parameter(self.layer2.fc_layer.weight.data * torch.Tensor(W))\n",
    "        self.layer3.fc_layer.weight = torch.nn.Parameter(self.layer3.fc_layer.weight.data * torch.Tensor(W))\n",
    "        self.layer4.fc_layer.weight = torch.nn.Parameter(self.layer4.fc_layer.weight.data * torch.Tensor(W))\n",
    "        self.layer5.fc_layer.weight = torch.nn.Parameter(self.layer5.fc_layer.weight.data * torch.Tensor(W))\n",
    "        self.layer6.fc_layer.weight = torch.nn.Parameter(self.layer6.fc_layer.weight.data * torch.Tensor(W))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "nb_inputs  = 700\n",
    "nb_hidden  = 200\n",
    "nb_outputs = 20\n",
    "\n",
    "time_step = 1e-3\n",
    "nb_steps = 100\n",
    "max_time = 1.4\n",
    "\n",
    "batch_size = 256\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'device: {device}')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available at: /Users/kerstjens/data/hdspikes/shd_train.h5\n",
      "Available at: /Users/kerstjens/data/hdspikes/shd_test.h5\n"
     ]
    }
   ],
   "source": [
    "# Here we load the Dataset\n",
    "cache_dir = os.path.expanduser(\"~/data\")\n",
    "cache_subdir = \"hdspikes\"\n",
    "heidel_utils.get_shd_dataset(cache_dir, cache_subdir)\n",
    "\n",
    "train_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'shd_train.h5'), 'r')\n",
    "test_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'shd_test.h5'), 'r')\n",
    "\n",
    "x_train = train_file['spikes']\n",
    "y_train = train_file['labels']\n",
    "x_test = test_file['spikes']\n",
    "y_test = test_file['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "legacy constructor expects device type: cpu but device type: cuda was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/kerstjens/msc_thesis/msc_thesis/heidel.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu4/Users/kerstjens/msc_thesis/msc_thesis/heidel.ipynb#ch0000007vscode-remote?line=0'>1</a>\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mFloatTensor(\u001b[39m3\u001b[39;49m,device\u001b[39m=\u001b[39;49mdevice)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: legacy constructor expects device type: cpu but device type: cuda was passed"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor(3,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_data_generator_from_hdf5_spikes(X, y, batch_size, nb_steps, nb_units, max_time, shuffle=True):\n",
    "    \"\"\" This generator takes a spike dataset and generates spiking network input as sparse tensors.\n",
    "\n",
    "    Args:\n",
    "        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\n",
    "        y: The labels\n",
    "    \"\"\"\n",
    "\n",
    "    labels_ = np.array(y,dtype=np.int)\n",
    "    number_of_batches = len(labels_)//batch_size\n",
    "    sample_index = np.arange(len(labels_))\n",
    "\n",
    "    # compute discrete firing times\n",
    "    firing_times = X['times']\n",
    "    units_fired = X['units']\n",
    "\n",
    "    time_bins = np.linspace(0, max_time, num=nb_steps)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "\n",
    "    total_batch_count = 0\n",
    "    counter = 0\n",
    "    while counter<number_of_batches:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "\n",
    "        coo = [ [] for i in range(3) ]\n",
    "        for bc,idx in enumerate(batch_index):\n",
    "            times = np.digitize(firing_times[idx], time_bins)\n",
    "            units = units_fired[idx]\n",
    "            batch = [bc for _ in range(len(times))]\n",
    "\n",
    "            coo[0].extend(batch)\n",
    "            coo[1].extend(times)\n",
    "            coo[2].extend(units)\n",
    "\n",
    "        i = torch.LongTensor(coo).to(device)\n",
    "        v = torch.FloatTensor(np.ones(len(coo[0]))).to(device)\n",
    "\n",
    "        X_batch = torch.sparse.FloatTensor(i, v, torch.Size([batch_size,nb_steps,nb_units])).to(device)\n",
    "        y_batch = torch.tensor(labels_[batch_index],device=device)\n",
    "\n",
    "        yield X_batch.to(device=device), y_batch.to(device=device)\n",
    "\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3073957/915459707.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  labels_ = np.array(y,dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchloss:  6.460285186767578\n",
      "time for batch: 0.1240544319152832\n",
      "batchloss:  6.25767183303833\n",
      "time for batch: 0.09685301780700684\n",
      "batchloss:  6.146482467651367\n",
      "time for batch: 0.1034243106842041\n",
      "batchloss:  6.026352882385254\n",
      "time for batch: 0.09610509872436523\n",
      "batchloss:  6.047924995422363\n",
      "time for batch: 0.10031318664550781\n",
      "batchloss:  5.9280266761779785\n",
      "time for batch: 0.09751772880554199\n",
      "batchloss:  5.726573467254639\n",
      "time for batch: 0.10065007209777832\n",
      "batchloss:  5.288479804992676\n",
      "time for batch: 0.1017463207244873\n",
      "batchloss:  5.63484525680542\n",
      "time for batch: 0.0983576774597168\n",
      "batchloss:  4.664196491241455\n",
      "time for batch: 0.1013491153717041\n",
      "batchloss:  5.157474517822266\n",
      "time for batch: 0.09750556945800781\n",
      "batchloss:  4.884401321411133\n",
      "time for batch: 0.09807848930358887\n",
      "batchloss:  5.005720138549805\n",
      "time for batch: 0.09674739837646484\n",
      "batchloss:  4.661982536315918\n",
      "time for batch: 0.09765243530273438\n",
      "batchloss:  4.52409553527832\n",
      "time for batch: 0.10457396507263184\n",
      "batchloss:  4.4209699630737305\n",
      "time for batch: 0.09911561012268066\n",
      "batchloss:  4.197897911071777\n",
      "time for batch: 0.10329532623291016\n",
      "batchloss:  3.8594367504119873\n",
      "time for batch: 0.10192704200744629\n",
      "batchloss:  4.420812129974365\n",
      "time for batch: 0.10434508323669434\n",
      "batchloss:  4.093761920928955\n",
      "time for batch: 0.0992896556854248\n",
      "batchloss:  3.7139575481414795\n",
      "time for batch: 0.09644556045532227\n",
      "batchloss:  4.370405197143555\n",
      "time for batch: 0.09841012954711914\n",
      "batchloss:  3.9835617542266846\n",
      "time for batch: 0.10490751266479492\n",
      "batchloss:  3.842677593231201\n",
      "time for batch: 0.09783291816711426\n",
      "batchloss:  3.7487950325012207\n",
      "time for batch: 0.09799838066101074\n",
      "batchloss:  3.660123109817505\n",
      "time for batch: 0.09643793106079102\n",
      "batchloss:  3.56473708152771\n",
      "time for batch: 0.10143303871154785\n",
      "batchloss:  3.709150791168213\n",
      "time for batch: 0.09623837471008301\n",
      "batchloss:  3.5201072692871094\n",
      "time for batch: 0.09713459014892578\n",
      "batchloss:  3.4311163425445557\n",
      "time for batch: 0.09998226165771484\n",
      "batchloss:  3.515129566192627\n",
      "time for batch: 0.09878826141357422\n",
      "time for epoch 0: 14.962013959884644\n",
      "epoch 0: \n",
      " loss: 3.515129566192627\n",
      "batchloss:  3.1940371990203857\n",
      "time for batch: 0.1029810905456543\n",
      "batchloss:  3.245861291885376\n",
      "time for batch: 0.07450485229492188\n",
      "batchloss:  3.4310142993927\n",
      "time for batch: 0.1007847785949707\n",
      "batchloss:  3.373138189315796\n",
      "time for batch: 0.10146260261535645\n",
      "batchloss:  3.107783794403076\n",
      "time for batch: 0.10143232345581055\n",
      "batchloss:  3.2214465141296387\n",
      "time for batch: 0.09763574600219727\n",
      "batchloss:  3.322193145751953\n",
      "time for batch: 0.09685659408569336\n",
      "batchloss:  3.1585333347320557\n",
      "time for batch: 0.09939885139465332\n",
      "batchloss:  3.257396936416626\n",
      "time for batch: 0.10408329963684082\n",
      "batchloss:  3.101247787475586\n",
      "time for batch: 0.0993342399597168\n",
      "batchloss:  3.2329699993133545\n",
      "time for batch: 0.09779024124145508\n",
      "batchloss:  2.9589180946350098\n",
      "time for batch: 0.09741759300231934\n",
      "batchloss:  3.1260623931884766\n",
      "time for batch: 0.1012122631072998\n",
      "batchloss:  3.02689790725708\n",
      "time for batch: 0.10175919532775879\n",
      "batchloss:  2.9934771060943604\n",
      "time for batch: 0.09821510314941406\n",
      "batchloss:  3.0147342681884766\n",
      "time for batch: 0.10275650024414062\n",
      "batchloss:  3.154453754425049\n",
      "time for batch: 0.10441946983337402\n",
      "batchloss:  3.058652639389038\n",
      "time for batch: 0.10551714897155762\n",
      "batchloss:  2.9599597454071045\n",
      "time for batch: 0.10436129570007324\n",
      "batchloss:  3.0775723457336426\n",
      "time for batch: 0.09867119789123535\n",
      "batchloss:  3.0118091106414795\n",
      "time for batch: 0.10705137252807617\n",
      "batchloss:  2.9210314750671387\n",
      "time for batch: 0.09906458854675293\n",
      "batchloss:  2.9217653274536133\n",
      "time for batch: 0.10028505325317383\n",
      "batchloss:  3.0125441551208496\n",
      "time for batch: 0.09774088859558105\n",
      "batchloss:  2.9827768802642822\n",
      "time for batch: 0.10516119003295898\n",
      "batchloss:  3.020803928375244\n",
      "time for batch: 0.09997820854187012\n",
      "batchloss:  2.953977346420288\n",
      "time for batch: 0.09928464889526367\n",
      "batchloss:  2.9416871070861816\n",
      "time for batch: 0.09976696968078613\n",
      "batchloss:  2.9726450443267822\n",
      "time for batch: 0.10035514831542969\n",
      "batchloss:  2.9573397636413574\n",
      "time for batch: 0.10471606254577637\n",
      "batchloss:  2.9404358863830566\n",
      "time for batch: 0.10570931434631348\n",
      "time for epoch 1: 14.5683913230896\n",
      "epoch 1: \n",
      " loss: 2.9404358863830566\n",
      "batchloss:  2.9887001514434814\n",
      "time for batch: 0.09988045692443848\n",
      "batchloss:  2.968813419342041\n",
      "time for batch: 0.07550239562988281\n",
      "batchloss:  2.958770990371704\n",
      "time for batch: 0.10442519187927246\n",
      "batchloss:  2.993060827255249\n",
      "time for batch: 0.10451030731201172\n",
      "batchloss:  2.9861321449279785\n",
      "time for batch: 0.1042490005493164\n",
      "batchloss:  2.983287811279297\n",
      "time for batch: 0.10539817810058594\n",
      "batchloss:  2.9389584064483643\n",
      "time for batch: 0.10742807388305664\n",
      "batchloss:  2.9544858932495117\n",
      "time for batch: 0.0985269546508789\n",
      "batchloss:  2.9142417907714844\n",
      "time for batch: 0.09925460815429688\n",
      "batchloss:  2.919614553451538\n",
      "time for batch: 0.09989261627197266\n",
      "batchloss:  2.9537713527679443\n",
      "time for batch: 0.10484814643859863\n",
      "batchloss:  2.9051594734191895\n",
      "time for batch: 0.10078001022338867\n",
      "batchloss:  2.946918249130249\n",
      "time for batch: 0.09942889213562012\n",
      "batchloss:  2.900378942489624\n",
      "time for batch: 0.0989837646484375\n",
      "batchloss:  2.973031759262085\n",
      "time for batch: 0.0984337329864502\n",
      "batchloss:  2.901780843734741\n",
      "time for batch: 0.10711002349853516\n",
      "batchloss:  2.914381504058838\n",
      "time for batch: 0.10345315933227539\n",
      "batchloss:  2.979978322982788\n",
      "time for batch: 0.09933757781982422\n",
      "batchloss:  2.883028984069824\n",
      "time for batch: 0.0993964672088623\n",
      "batchloss:  2.947819232940674\n",
      "time for batch: 0.10062646865844727\n",
      "batchloss:  2.938345193862915\n",
      "time for batch: 0.10173344612121582\n",
      "batchloss:  2.923222541809082\n",
      "time for batch: 0.09970331192016602\n",
      "batchloss:  2.9242851734161377\n",
      "time for batch: 0.099853515625\n",
      "batchloss:  2.9348511695861816\n",
      "time for batch: 0.10336875915527344\n",
      "batchloss:  2.9676923751831055\n",
      "time for batch: 0.10644769668579102\n",
      "batchloss:  2.9053022861480713\n",
      "time for batch: 0.09816884994506836\n",
      "batchloss:  2.9359958171844482\n",
      "time for batch: 0.10052633285522461\n",
      "batchloss:  2.967102527618408\n",
      "time for batch: 0.10670304298400879\n",
      "batchloss:  2.9351563453674316\n",
      "time for batch: 0.09931135177612305\n",
      "batchloss:  2.9207451343536377\n",
      "time for batch: 0.1085045337677002\n",
      "batchloss:  2.898082733154297\n",
      "time for batch: 0.09971046447753906\n",
      "time for epoch 2: 14.455029010772705\n",
      "epoch 2: \n",
      " loss: 2.898082733154297\n",
      "batchloss:  2.9211301803588867\n",
      "time for batch: 0.10959792137145996\n",
      "batchloss:  2.899080753326416\n",
      "time for batch: 0.0753777027130127\n",
      "batchloss:  2.974311351776123\n",
      "time for batch: 0.10563135147094727\n",
      "batchloss:  2.941605806350708\n",
      "time for batch: 0.10067582130432129\n",
      "batchloss:  2.8689041137695312\n",
      "time for batch: 0.09893488883972168\n",
      "batchloss:  2.9621899127960205\n",
      "time for batch: 0.10111141204833984\n",
      "batchloss:  2.912935733795166\n",
      "time for batch: 0.10055923461914062\n",
      "batchloss:  2.8941845893859863\n",
      "time for batch: 0.10051178932189941\n",
      "batchloss:  2.895271062850952\n",
      "time for batch: 0.10011529922485352\n",
      "batchloss:  2.869786262512207\n",
      "time for batch: 0.10000324249267578\n",
      "batchloss:  2.923772096633911\n",
      "time for batch: 0.10035467147827148\n",
      "batchloss:  2.8882176876068115\n",
      "time for batch: 0.11023640632629395\n",
      "batchloss:  2.907959461212158\n",
      "time for batch: 0.09952592849731445\n",
      "batchloss:  2.913742780685425\n",
      "time for batch: 0.10506939888000488\n",
      "batchloss:  2.9120583534240723\n",
      "time for batch: 0.10090017318725586\n",
      "batchloss:  2.9159035682678223\n",
      "time for batch: 0.1066746711730957\n",
      "batchloss:  2.90630841255188\n",
      "time for batch: 0.1027376651763916\n",
      "batchloss:  2.896649122238159\n",
      "time for batch: 0.10183405876159668\n",
      "batchloss:  2.902585983276367\n",
      "time for batch: 0.10081744194030762\n",
      "batchloss:  2.8869481086730957\n",
      "time for batch: 0.10145998001098633\n",
      "batchloss:  2.898648262023926\n",
      "time for batch: 0.10933995246887207\n",
      "batchloss:  2.898611545562744\n",
      "time for batch: 0.10322809219360352\n",
      "batchloss:  2.9090044498443604\n",
      "time for batch: 0.10970044136047363\n",
      "batchloss:  2.8755815029144287\n",
      "time for batch: 0.11208176612854004\n",
      "batchloss:  2.9141173362731934\n",
      "time for batch: 0.10190844535827637\n",
      "batchloss:  2.8849711418151855\n",
      "time for batch: 0.10944294929504395\n",
      "batchloss:  2.944246530532837\n",
      "time for batch: 0.10151553153991699\n",
      "batchloss:  2.8587348461151123\n",
      "time for batch: 0.1018075942993164\n",
      "batchloss:  2.9290943145751953\n",
      "time for batch: 0.10272097587585449\n",
      "batchloss:  2.879030704498291\n",
      "time for batch: 0.10287332534790039\n",
      "batchloss:  2.888701915740967\n",
      "time for batch: 0.10341334342956543\n",
      "time for epoch 3: 14.624475717544556\n",
      "epoch 3: \n",
      " loss: 2.888701915740967\n",
      "batchloss:  2.8975837230682373\n",
      "time for batch: 0.10208535194396973\n",
      "batchloss:  2.912923574447632\n",
      "time for batch: 0.07514047622680664\n",
      "batchloss:  2.866379737854004\n",
      "time for batch: 0.10096549987792969\n",
      "batchloss:  2.851966619491577\n",
      "time for batch: 0.10310006141662598\n",
      "batchloss:  2.9281556606292725\n",
      "time for batch: 0.11172366142272949\n",
      "batchloss:  2.9244556427001953\n",
      "time for batch: 0.1036992073059082\n",
      "batchloss:  2.9244632720947266\n",
      "time for batch: 0.10393524169921875\n",
      "batchloss:  2.83264422416687\n",
      "time for batch: 0.10426473617553711\n",
      "batchloss:  2.8665056228637695\n",
      "time for batch: 0.10239386558532715\n",
      "batchloss:  2.8288791179656982\n",
      "time for batch: 0.10270953178405762\n",
      "batchloss:  2.8982768058776855\n",
      "time for batch: 0.10507607460021973\n",
      "batchloss:  2.8647305965423584\n",
      "time for batch: 0.10146212577819824\n",
      "batchloss:  2.947509765625\n",
      "time for batch: 0.10954761505126953\n",
      "batchloss:  2.9302897453308105\n",
      "time for batch: 0.10290122032165527\n",
      "batchloss:  2.882436752319336\n",
      "time for batch: 0.11113667488098145\n",
      "batchloss:  2.906062602996826\n",
      "time for batch: 0.10199952125549316\n",
      "batchloss:  2.8668055534362793\n",
      "time for batch: 0.10260438919067383\n",
      "batchloss:  2.853550434112549\n",
      "time for batch: 0.10096216201782227\n",
      "batchloss:  2.847874164581299\n",
      "time for batch: 0.10969090461730957\n",
      "batchloss:  2.8441240787506104\n",
      "time for batch: 0.10145688056945801\n",
      "batchloss:  2.899081230163574\n",
      "time for batch: 0.10167884826660156\n",
      "batchloss:  2.8800227642059326\n",
      "time for batch: 0.10198092460632324\n",
      "batchloss:  2.8950352668762207\n",
      "time for batch: 0.10298371315002441\n",
      "batchloss:  2.9230237007141113\n",
      "time for batch: 0.10478663444519043\n",
      "batchloss:  2.8484439849853516\n",
      "time for batch: 0.1081843376159668\n",
      "batchloss:  2.8642003536224365\n",
      "time for batch: 0.1024017333984375\n",
      "batchloss:  2.9126734733581543\n",
      "time for batch: 0.10422635078430176\n",
      "batchloss:  2.8938229084014893\n",
      "time for batch: 0.10277938842773438\n",
      "batchloss:  2.9082517623901367\n",
      "time for batch: 0.10306692123413086\n",
      "batchloss:  2.848005533218384\n",
      "time for batch: 0.11051821708679199\n",
      "batchloss:  2.8516616821289062\n",
      "time for batch: 0.10239863395690918\n",
      "time for epoch 4: 14.566680908203125\n",
      "epoch 4: \n",
      " loss: 2.8516616821289062\n",
      "batchloss:  2.88647723197937\n",
      "time for batch: 0.10819816589355469\n",
      "batchloss:  2.9131052494049072\n",
      "time for batch: 0.0752115249633789\n",
      "batchloss:  2.826017141342163\n",
      "time for batch: 0.10894155502319336\n",
      "batchloss:  2.9248857498168945\n",
      "time for batch: 0.10595941543579102\n",
      "batchloss:  2.824052095413208\n",
      "time for batch: 0.10366225242614746\n",
      "batchloss:  2.846871852874756\n",
      "time for batch: 0.10282516479492188\n",
      "batchloss:  2.827357053756714\n",
      "time for batch: 0.11319804191589355\n",
      "batchloss:  2.943606376647949\n",
      "time for batch: 0.10355806350708008\n",
      "batchloss:  2.8214409351348877\n",
      "time for batch: 0.1027534008026123\n",
      "batchloss:  2.89056658744812\n",
      "time for batch: 0.1027684211730957\n",
      "batchloss:  2.8401384353637695\n",
      "time for batch: 0.10939550399780273\n",
      "batchloss:  2.8251307010650635\n",
      "time for batch: 0.10262894630432129\n",
      "batchloss:  2.84684157371521\n",
      "time for batch: 0.10190558433532715\n",
      "batchloss:  2.8859612941741943\n",
      "time for batch: 0.10523319244384766\n",
      "batchloss:  2.846909999847412\n",
      "time for batch: 0.10401248931884766\n",
      "batchloss:  2.881526470184326\n",
      "time for batch: 0.10348391532897949\n",
      "batchloss:  2.8220138549804688\n",
      "time for batch: 0.10333538055419922\n",
      "batchloss:  2.8434934616088867\n",
      "time for batch: 0.10338425636291504\n",
      "batchloss:  2.8748490810394287\n",
      "time for batch: 0.10807037353515625\n",
      "batchloss:  2.87924861907959\n",
      "time for batch: 0.10502219200134277\n",
      "batchloss:  2.841343879699707\n",
      "time for batch: 0.10354113578796387\n",
      "batchloss:  2.8144729137420654\n",
      "time for batch: 0.10935330390930176\n",
      "batchloss:  2.8648548126220703\n",
      "time for batch: 0.10561895370483398\n",
      "batchloss:  2.8823330402374268\n",
      "time for batch: 0.11337757110595703\n",
      "batchloss:  2.8783586025238037\n",
      "time for batch: 0.1050257682800293\n",
      "batchloss:  2.7920682430267334\n",
      "time for batch: 0.10275459289550781\n",
      "batchloss:  2.830162286758423\n",
      "time for batch: 0.10381436347961426\n",
      "batchloss:  2.811091423034668\n",
      "time for batch: 0.1048588752746582\n",
      "batchloss:  2.888624668121338\n",
      "time for batch: 0.10954594612121582\n",
      "batchloss:  2.889585018157959\n",
      "time for batch: 0.10344982147216797\n",
      "batchloss:  2.8588900566101074\n",
      "time for batch: 0.10441088676452637\n",
      "time for epoch 5: 14.325175762176514\n",
      "epoch 5: \n",
      " loss: 2.8588900566101074\n",
      "batchloss:  2.828484535217285\n",
      "time for batch: 0.10452508926391602\n",
      "batchloss:  2.811164140701294\n",
      "time for batch: 0.08034515380859375\n",
      "batchloss:  2.8323440551757812\n",
      "time for batch: 0.10388875007629395\n",
      "batchloss:  2.8615691661834717\n",
      "time for batch: 0.10441136360168457\n",
      "batchloss:  2.8273892402648926\n",
      "time for batch: 0.10462427139282227\n",
      "batchloss:  2.8564465045928955\n",
      "time for batch: 0.10587501525878906\n",
      "batchloss:  2.842827796936035\n",
      "time for batch: 0.10981059074401855\n",
      "batchloss:  2.8268392086029053\n",
      "time for batch: 0.10419034957885742\n",
      "batchloss:  2.8059215545654297\n",
      "time for batch: 0.10955119132995605\n",
      "batchloss:  2.839801788330078\n",
      "time for batch: 0.11138677597045898\n",
      "batchloss:  2.808742046356201\n",
      "time for batch: 0.10552334785461426\n",
      "batchloss:  2.848731279373169\n",
      "time for batch: 0.10344624519348145\n",
      "batchloss:  2.784815549850464\n",
      "time for batch: 0.10406231880187988\n",
      "batchloss:  2.8348634243011475\n",
      "time for batch: 0.10341930389404297\n",
      "batchloss:  2.8123416900634766\n",
      "time for batch: 0.11026763916015625\n",
      "batchloss:  2.800377607345581\n",
      "time for batch: 0.1081700325012207\n",
      "batchloss:  2.8560800552368164\n",
      "time for batch: 0.10531973838806152\n",
      "batchloss:  2.838383436203003\n",
      "time for batch: 0.11228346824645996\n",
      "batchloss:  2.843444585800171\n",
      "time for batch: 0.10305666923522949\n",
      "batchloss:  2.8690738677978516\n",
      "time for batch: 0.1094057559967041\n",
      "batchloss:  2.841796875\n",
      "time for batch: 0.1069486141204834\n",
      "batchloss:  2.741891622543335\n",
      "time for batch: 0.1047825813293457\n",
      "batchloss:  2.8424699306488037\n",
      "time for batch: 0.10624432563781738\n",
      "batchloss:  2.8147835731506348\n",
      "time for batch: 0.10477852821350098\n",
      "batchloss:  2.854674816131592\n",
      "time for batch: 0.10556387901306152\n",
      "batchloss:  2.8042614459991455\n",
      "time for batch: 0.10392880439758301\n",
      "batchloss:  2.8419229984283447\n",
      "time for batch: 0.10543680191040039\n",
      "batchloss:  2.797102928161621\n",
      "time for batch: 0.10422515869140625\n",
      "batchloss:  2.8319344520568848\n",
      "time for batch: 0.1041100025177002\n",
      "batchloss:  2.8061459064483643\n",
      "time for batch: 0.10587286949157715\n",
      "batchloss:  2.8563313484191895\n",
      "time for batch: 0.10368466377258301\n",
      "time for epoch 6: 14.46286153793335\n",
      "epoch 6: \n",
      " loss: 2.8563313484191895\n",
      "batchloss:  2.7846758365631104\n",
      "time for batch: 0.1066431999206543\n",
      "batchloss:  2.7847514152526855\n",
      "time for batch: 0.08279228210449219\n",
      "batchloss:  2.7525546550750732\n",
      "time for batch: 0.10355067253112793\n",
      "batchloss:  2.866447687149048\n",
      "time for batch: 0.10754013061523438\n",
      "batchloss:  2.7920525074005127\n",
      "time for batch: 0.10418343544006348\n",
      "batchloss:  2.7887368202209473\n",
      "time for batch: 0.11419391632080078\n",
      "batchloss:  2.8489737510681152\n",
      "time for batch: 0.11049890518188477\n",
      "batchloss:  2.813509702682495\n",
      "time for batch: 0.1063852310180664\n",
      "batchloss:  2.8178725242614746\n",
      "time for batch: 0.10751080513000488\n",
      "batchloss:  2.7719812393188477\n",
      "time for batch: 0.10605764389038086\n",
      "batchloss:  2.8484222888946533\n",
      "time for batch: 0.11142992973327637\n",
      "batchloss:  2.7861504554748535\n",
      "time for batch: 0.11256718635559082\n",
      "batchloss:  2.820319890975952\n",
      "time for batch: 0.11103439331054688\n",
      "batchloss:  2.7678585052490234\n",
      "time for batch: 0.11276865005493164\n",
      "batchloss:  2.845940113067627\n",
      "time for batch: 0.10522103309631348\n",
      "batchloss:  2.7499938011169434\n",
      "time for batch: 0.11080431938171387\n",
      "batchloss:  2.7830095291137695\n",
      "time for batch: 0.10527324676513672\n",
      "batchloss:  2.8243486881256104\n",
      "time for batch: 0.10542106628417969\n",
      "batchloss:  2.8057382106781006\n",
      "time for batch: 0.1074838638305664\n",
      "batchloss:  2.786252498626709\n",
      "time for batch: 0.11015892028808594\n",
      "batchloss:  2.7576353549957275\n",
      "time for batch: 0.10820698738098145\n",
      "batchloss:  2.8224382400512695\n",
      "time for batch: 0.11097574234008789\n",
      "batchloss:  2.789541244506836\n",
      "time for batch: 0.11360311508178711\n",
      "batchloss:  2.801945447921753\n",
      "time for batch: 0.10526275634765625\n",
      "batchloss:  2.787803888320923\n",
      "time for batch: 0.10505151748657227\n",
      "batchloss:  2.8042733669281006\n",
      "time for batch: 0.11068058013916016\n",
      "batchloss:  2.796290159225464\n",
      "time for batch: 0.11331915855407715\n",
      "batchloss:  2.7818408012390137\n",
      "time for batch: 0.11179995536804199\n",
      "batchloss:  2.8610382080078125\n",
      "time for batch: 0.11548590660095215\n",
      "batchloss:  2.7860190868377686\n",
      "time for batch: 0.11505722999572754\n",
      "batchloss:  2.8564958572387695\n",
      "time for batch: 0.11063265800476074\n",
      "time for epoch 7: 14.599168539047241\n",
      "epoch 7: \n",
      " loss: 2.8564958572387695\n",
      "batchloss:  2.785910129547119\n",
      "time for batch: 0.10762262344360352\n",
      "batchloss:  2.8105947971343994\n",
      "time for batch: 0.0803825855255127\n",
      "batchloss:  2.7991814613342285\n",
      "time for batch: 0.10702776908874512\n",
      "batchloss:  2.7705886363983154\n",
      "time for batch: 0.11197257041931152\n",
      "batchloss:  2.7327067852020264\n",
      "time for batch: 0.10583066940307617\n",
      "batchloss:  2.7400121688842773\n",
      "time for batch: 0.11466288566589355\n",
      "batchloss:  2.7520833015441895\n",
      "time for batch: 0.11026978492736816\n",
      "batchloss:  2.7613525390625\n",
      "time for batch: 0.11202669143676758\n",
      "batchloss:  2.8093388080596924\n",
      "time for batch: 0.10734438896179199\n",
      "batchloss:  2.7783846855163574\n",
      "time for batch: 0.10971975326538086\n",
      "batchloss:  2.8070108890533447\n",
      "time for batch: 0.1057133674621582\n",
      "batchloss:  2.811305522918701\n",
      "time for batch: 0.11494684219360352\n",
      "batchloss:  2.8567113876342773\n",
      "time for batch: 0.10953235626220703\n",
      "batchloss:  2.756014347076416\n",
      "time for batch: 0.11321449279785156\n",
      "batchloss:  2.7149391174316406\n",
      "time for batch: 0.1128702163696289\n",
      "batchloss:  2.760892629623413\n",
      "time for batch: 0.11146020889282227\n",
      "batchloss:  2.7452757358551025\n",
      "time for batch: 0.10594701766967773\n",
      "batchloss:  2.813210964202881\n",
      "time for batch: 0.1067197322845459\n",
      "batchloss:  2.7097389698028564\n",
      "time for batch: 0.11129999160766602\n",
      "batchloss:  2.819178819656372\n",
      "time for batch: 0.11310815811157227\n",
      "batchloss:  2.7749850749969482\n",
      "time for batch: 0.11146306991577148\n",
      "batchloss:  2.8042805194854736\n",
      "time for batch: 0.10633683204650879\n",
      "batchloss:  2.8356997966766357\n",
      "time for batch: 0.1144418716430664\n",
      "batchloss:  2.818690299987793\n",
      "time for batch: 0.11419320106506348\n",
      "batchloss:  2.754012107849121\n",
      "time for batch: 0.10448098182678223\n",
      "batchloss:  2.780702829360962\n",
      "time for batch: 0.1119842529296875\n",
      "batchloss:  2.738417863845825\n",
      "time for batch: 0.10522747039794922\n",
      "batchloss:  2.737090826034546\n",
      "time for batch: 0.11159944534301758\n",
      "batchloss:  2.7567336559295654\n",
      "time for batch: 0.11099624633789062\n",
      "batchloss:  2.769315004348755\n",
      "time for batch: 0.1106564998626709\n",
      "batchloss:  2.8041083812713623\n",
      "time for batch: 0.10553431510925293\n",
      "time for epoch 8: 14.57461142539978\n",
      "epoch 8: \n",
      " loss: 2.8041083812713623\n",
      "batchloss:  2.7743139266967773\n",
      "time for batch: 0.11565494537353516\n",
      "batchloss:  2.707916498184204\n",
      "time for batch: 0.08077597618103027\n",
      "batchloss:  2.7238736152648926\n",
      "time for batch: 0.1062159538269043\n",
      "batchloss:  2.787870168685913\n",
      "time for batch: 0.10663223266601562\n",
      "batchloss:  2.7304394245147705\n",
      "time for batch: 0.11155056953430176\n",
      "batchloss:  2.7802846431732178\n",
      "time for batch: 0.11331868171691895\n",
      "batchloss:  2.744464874267578\n",
      "time for batch: 0.10550189018249512\n",
      "batchloss:  2.804908275604248\n",
      "time for batch: 0.10980725288391113\n",
      "batchloss:  2.7408928871154785\n",
      "time for batch: 0.11328291893005371\n",
      "batchloss:  2.727207899093628\n",
      "time for batch: 0.10587596893310547\n",
      "batchloss:  2.7602286338806152\n",
      "time for batch: 0.11118769645690918\n",
      "batchloss:  2.707475185394287\n",
      "time for batch: 0.11479926109313965\n",
      "batchloss:  2.7877790927886963\n",
      "time for batch: 0.11318850517272949\n",
      "batchloss:  2.7535862922668457\n",
      "time for batch: 0.1135859489440918\n",
      "batchloss:  2.728667736053467\n",
      "time for batch: 0.2165677547454834\n",
      "batchloss:  2.778726577758789\n",
      "time for batch: 0.11319541931152344\n",
      "batchloss:  2.829566717147827\n",
      "time for batch: 0.11322307586669922\n",
      "batchloss:  2.7474498748779297\n",
      "time for batch: 0.10732889175415039\n",
      "batchloss:  2.6816868782043457\n",
      "time for batch: 0.10906100273132324\n",
      "batchloss:  2.708641290664673\n",
      "time for batch: 0.111663818359375\n",
      "batchloss:  2.7607786655426025\n",
      "time for batch: 0.1122286319732666\n",
      "batchloss:  2.7158234119415283\n",
      "time for batch: 0.11283588409423828\n",
      "batchloss:  2.7512807846069336\n",
      "time for batch: 0.10723638534545898\n",
      "batchloss:  2.778024435043335\n",
      "time for batch: 0.11441850662231445\n",
      "batchloss:  2.744121551513672\n",
      "time for batch: 0.11693620681762695\n",
      "batchloss:  2.696885585784912\n",
      "time for batch: 0.11101078987121582\n",
      "batchloss:  2.7114336490631104\n",
      "time for batch: 0.11184120178222656\n",
      "batchloss:  2.692864418029785\n",
      "time for batch: 0.11185121536254883\n",
      "batchloss:  2.7754995822906494\n",
      "time for batch: 0.10651803016662598\n",
      "batchloss:  2.772416830062866\n",
      "time for batch: 0.11759614944458008\n",
      "batchloss:  2.7195661067962646\n",
      "time for batch: 0.10762739181518555\n",
      "time for epoch 9: 14.856037855148315\n",
      "epoch 9: \n",
      " loss: 2.7195661067962646\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model = OneHiddenModel(in_channels=nb_inputs,hidden_channels=nb_hidden,out_channels=nb_outputs,batch_size=batch_size,W=None).to(device)\n",
    "\n",
    "ce_loss = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "params = model.parameters()\n",
    "opt = torch.optim.Adam(params, lr=1e-4, betas=[0., .95]) #lr is the learning rate\n",
    "decay = .9\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    avg_loss = 0\n",
    "    model.train()\n",
    "    sum_acc=0\n",
    "    #batches\n",
    "    for x,y in sparse_data_generator_from_hdf5_spikes(x_train, y_train, batch_size, nb_steps, nb_units=nb_inputs, max_time=max_time, shuffle=True):\n",
    "        tik = time.time()\n",
    "        model.init_states()\n",
    "\n",
    "        Sprobe = torch.zeros((batch_size,model.out_channels),device=device)\n",
    "\n",
    "        #timesteps\n",
    "        for n in range(nb_steps):\n",
    "            out_state = model(x.to_dense()[:,n])\n",
    "\n",
    "            #add decay for leakiness\n",
    "            #collect spikes over time\n",
    "            Sprobe = decay * Sprobe + out_state.S\n",
    "\n",
    "\n",
    "        prediction = Sprobe\n",
    "        #if i==0:\n",
    "        #    print(prediction)\n",
    "\n",
    "        #accuracy = val_accuracy(prediction,y_train[train_batch_ids[i]])\n",
    "\n",
    "        #class labels only\n",
    "        loss = ce_loss(prediction,y)\n",
    "\n",
    "        print(f'batchloss:  {loss}')\n",
    "\n",
    "        #tonic & torch neuromorphic\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        #model.init_mod_weights(W2)\n",
    "        #sum_acc = sum_acc + accuracy\n",
    "        #sum_loss = sum_loss + loss\n",
    "        count = 0\n",
    "        avg_loss = ((avg_loss * count) + loss)/(count+1)\n",
    "        count += 1\n",
    "    #avg_train_acc = sum_acc/(len(train_batch_ids))\n",
    "    #avg_loss = sum_loss/(len(train_batch_ids))\n",
    "    #if i == 3:/'\n",
    "    #loss_hist = loss_hist + [float(avg_loss)]\n",
    "    #acc_hist = acc_hist + [float(avg_train_acc)]\n",
    "        tok = time.time()\n",
    "        print(f'time for batch: {tok-tik}')\n",
    "    toc = time.time()\n",
    "    print(f'time for epoch {epoch}: {toc-tic}')\n",
    "    if epoch%1==0:\n",
    "        print(f'epoch {epoch}: \\n loss: {avg_loss}')\n",
    "        #print(f'train_acc: {avg_train_acc}')\n",
    "    #val_acc,_,_ = validation_acc(X_test,y_test,model,test_batch_ids)\n",
    "    #val_acc_hist = val_acc_hist + [val_acc]\n",
    "    #print(f'val_acc: {val_acc}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6200234780f1abd365c930dddb95b38298813a9988d77a8785e770f4fc612d5b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
